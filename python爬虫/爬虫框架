
scrapy框架
    创建项目：scrapy startproject demo_1
    创建爬虫：scrapy genspider spiders www.xxx.com
    运行爬虫：scrapy crawl spiders
    命令行操作：scrapy crawl spiders -o a.json  a.csv  a.txt

    # 项目文件夹 必须作为根目录打开  否则一些文件无法引入
    demo_1 基础设置
    demo_2 scrapy + selenium 自动化
    demo_3 scrapy + scrapy_redis 分布式

    settings.py -- 内部的默认设置
    items.py    -- 确定爬取的目标item{字典}
    spiders.py  -- 引入item 解析网页数据 将item返回给pipelines.py
    pipelines.py - 数据的持久化存储 mysql redis mongodb 数据库存储  本地文件-文本/二进制文件
    middlewares.py 下载器中间件 对request-UA|proxy response-selenium动态加载   进行拦截并修改

    针对ajax请求和js渲染的网页  可以是 scrapy中间件 + selenium的结合使用
    在scrapy的下载器中间件内，
        request请求，还可以设置UA头部，包含headers 以及ip代理
        response响应， 使用到selenium自动化测试模块
        exception异常， 对异常的处理

    分布式scrapy scrapy_redis  settings.py参数设置 redis服务端客户端的启用  lpush test_list 'http://...'

    demo_3  需要进一步的加强


分布式网络爬虫 - scrapy-redis
定向爬取的爬虫
深度爬取的爬虫
数据的挖掘
